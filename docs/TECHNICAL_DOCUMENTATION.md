# DocumentaÃ§Ã£o TÃ©cnica - Ship Lineup Data Pipeline

## VisÃ£o Geral da Arquitetura

### Arquitetura Medallion

O sistema implementa a arquitetura medallion com trÃªs camadas distintas:

#### Bronze Layer (Dados Brutos)
- **PropÃ³sito**: Armazenamento de dados brutos coletados diretamente das fontes
- **CaracterÃ­sticas**:
  - Dados nÃ£o processados
  - Metadados de coleta (timestamp, fonte, etc.)
  - PreservaÃ§Ã£o da estrutura original
  - Formato: Parquet + SQLite/PostgreSQL

#### Silver Layer (Dados Processados)
- **PropÃ³sito**: Dados limpos, padronizados e enriquecidos
- **TransformaÃ§Ãµes**:
  - Limpeza de dados (remover duplicatas, valores nulos)
  - PadronizaÃ§Ã£o (nomes de portos, direÃ§Ãµes, produtos)
  - Enriquecimento (classificaÃ§Ãµes, categorias)
  - ValidaÃ§Ã£o de qualidade

#### Gold Layer (Dados de NegÃ³cio)
- **PropÃ³sito**: Dados prontos para anÃ¡lise e relatÃ³rios
- **CaracterÃ­sticas**:
  - MÃ©tricas de negÃ³cio
  - AgregaÃ§Ãµes temporais
  - Indicadores de performance
  - Dados otimizados para consulta

## Componentes do Sistema

### 1. Coletores de Dados (Data Collectors)

#### BaseCollector
```python
class BaseCollector(ABC):
    """Classe base abstrata para todos os coletores"""
    
    def __init__(self, port_name: str, port_code: str)
    def collect_data(self, start_date: str, end_date: str) -> pd.DataFrame
    def make_request(self, url: str, params: Optional[Dict] = None) -> requests.Response
    def standardize_data(self, df: pd.DataFrame) -> pd.DataFrame
    def validate_data(self, df: pd.DataFrame) -> pd.DataFrame
```

**CaracterÃ­sticas**:
- Classe abstrata para padronizaÃ§Ã£o
- Tratamento de erros e retry automÃ¡tico
- ValidaÃ§Ã£o bÃ¡sica de dados
- Metadados de coleta

#### ParanaguaCollector
- **Fonte**: APPA (AdministraÃ§Ã£o dos Portos de ParanaguÃ¡ e Antonina)
- **URL**: https://www.appaweb.appa.pr.gov.br/appaweb/pesquisa.aspx?WCI=relLineUpRetroativo
- **MÃ©todo**: Web scraping com BeautifulSoup
- **Dados**: Lineup retroativo de navios

#### SantosCollector
- **Fonte**: Codesp (Companhia Docas do Estado de SÃ£o Paulo)
- **URL**: https://www.portodesantos.com.br/informacoes-operacionais/operacoes-portuarias/navegacao-e-movimento-de-navios/navios-esperados-carga/
- **MÃ©todo**: Web scraping + APIs alternativas
- **Dados**: Navios esperados com carga

### 2. Pipeline ETL (MedallionPipeline)

#### Processamento Bronze â†’ Silver
```python
def process_silver_layer(self, bronze_file: str) -> str:
    # 1. Carregar dados bronze
    # 2. Limpeza de dados
    # 3. PadronizaÃ§Ã£o
    # 4. Enriquecimento
    # 5. Salvar silver
```

**TransformaÃ§Ãµes Silver**:
- RemoÃ§Ã£o de duplicatas
- Tratamento de valores nulos
- PadronizaÃ§Ã£o de texto (maiÃºsculas, trim)
- ConversÃ£o de tipos de dados
- ClassificaÃ§Ã£o de produtos e navios
- ExtraÃ§Ã£o de campos temporais

#### Processamento Silver â†’ Gold
```python
def process_gold_layer(self, silver_file: str) -> str:
    # 1. Carregar dados silver
    # 2. Aplicar regras de negÃ³cio
    # 3. Criar agregaÃ§Ãµes
    # 4. Calcular mÃ©tricas
    # 5. Salvar gold
```

**TransformaÃ§Ãµes Gold**:
- AplicaÃ§Ã£o de regras de negÃ³cio
- CÃ¡lculo de mÃ©tricas (mÃ©dias mÃ³veis, crescimento)
- CriaÃ§Ã£o de rankings
- Flags de qualidade
- AgregaÃ§Ãµes por perÃ­odo/produto/porto

### 3. Gerenciador de Banco de Dados (DatabaseManager)

#### Estrutura das Tabelas

**Bronze Table**:
```sql
CREATE TABLE bronze_ship_lineup (
    id INTEGER PRIMARY KEY,
    porto VARCHAR(50),
    navio VARCHAR(100),
    produto VARCHAR(100),
    sentido VARCHAR(20),
    volume FLOAT,
    data_chegada DATETIME,
    data_partida DATETIME,
    armador VARCHAR(100),
    agente VARCHAR(100),
    collection_date DATETIME,
    source VARCHAR(50),
    processing_timestamp DATETIME,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

**Silver Table** (campos adicionais):
```sql
-- Campos da bronze +
ano INTEGER,
mes INTEGER,
dia_semana VARCHAR(20),
trimestre INTEGER,
tipo_navio VARCHAR(50),
categoria_produto VARCHAR(50),
categoria_volume VARCHAR(20),
status_operacao VARCHAR(20),
flag_qualidade VARCHAR(20)
```

**Gold Table** (campos adicionais):
```sql
-- Campos da silver +
volume_ma_7d FLOAT,
volume_ma_30d FLOAT,
crescimento_volume FLOAT,
ranking_volume FLOAT
```

#### OperaÃ§Ãµes Principais
- `insert_bronze_data()`: InserÃ§Ã£o de dados brutos
- `insert_silver_data()`: InserÃ§Ã£o de dados processados
- `insert_gold_data()`: InserÃ§Ã£o de dados de negÃ³cio
- `get_latest_collection_date()`: Ãšltima coleta por fonte
- `get_data_by_date_range()`: Consulta por perÃ­odo
- `get_aggregated_data()`: Dados agregados
- `get_data_quality_report()`: RelatÃ³rio de qualidade

### 4. Agendador (DailyScheduler)

#### Funcionalidades
- **Coleta DiÃ¡ria**: ExecuÃ§Ã£o diÃ¡ria Ã s 06:00
- **AtualizaÃ§Ã£o Incremental**: ExecuÃ§Ã£o Ã s 10:00, 14:00, 18:00
- **Limpeza Semanal**: ExecuÃ§Ã£o domingos Ã s 02:00
- **Coleta Manual**: Para perÃ­odos especÃ­ficos

#### Fluxo de ExecuÃ§Ã£o
```python
def run_daily_collection(self):
    # 1. Calcular perÃ­odo (Ãºltimos 7 dias)
    # 2. Coletar dados de ambas as fontes
    # 3. Processar atravÃ©s das camadas medallion
    # 4. Inserir no banco de dados
    # 5. Gerar relatÃ³rio diÃ¡rio
```

### 5. ValidaÃ§Ã£o de Dados (DataValidator)

#### Regras de ValidaÃ§Ã£o
- **Campos ObrigatÃ³rios**: porto, navio, produto, sentido, data_chegada
- **Valores VÃ¡lidos**: Portos (PARANAGUÃ, SANTOS), Sentidos (EXPORTAÃ‡ÃƒO, IMPORTAÃ‡ÃƒO)
- **Faixas de Volume**: 0 a 1.000.000 toneladas
- **Datas**: ValidaÃ§Ã£o de formato e consistÃªncia temporal

#### MÃ©tricas de Qualidade
- **Data Quality Score**: Percentual de registros vÃ¡lidos
- **Flags de Qualidade**: OK, VOLUME_BAIXO, VOLUME_ALTO, DATA_FUTURA
- **DetecÃ§Ã£o de Anomalias**: Valores fora do padrÃ£o estatÃ­stico

### 6. DicionÃ¡rio de Dados (DataDictionary)

#### ClassificaÃ§Ãµes AutomÃ¡ticas

**Produtos**:
- GRÃƒOS: SOJA, MILHO, TRIGO, ARROZ
- AÃ‡ÃšCAR: AÃ‡ÃšCAR, SUGAR
- FERTILIZANTES: FERTILIZANTE, UREIA, FOSFATO
- CONTAINER: CONTAINER, CONTÃŠINER
- MINÃ‰RIOS: MINÃ‰RIO, IRON ORE
- OUTROS: Categoria catch-all

**Tipos de Navio**:
- CARGA_GERAL: BULK, GRAIN, CARGO
- CONTAINER: CONTAINER, BOX
- TANQUE: TANKER, OIL, PETROLEUM
- RO-RO: RO-RO, FERRY
- OUTROS: Categoria catch-all

## Fluxo de Dados

### 1. Coleta
```
Fonte Web â†’ Coletor â†’ Dados Brutos â†’ ValidaÃ§Ã£o BÃ¡sica
```

### 2. Processamento Bronze
```
Dados Brutos â†’ Metadados â†’ Armazenamento Bronze â†’ Arquivo Parquet
```

### 3. Processamento Silver
```
Bronze â†’ Limpeza â†’ PadronizaÃ§Ã£o â†’ Enriquecimento â†’ Silver
```

### 4. Processamento Gold
```
Silver â†’ Regras NegÃ³cio â†’ AgregaÃ§Ãµes â†’ MÃ©tricas â†’ Gold
```

### 5. Armazenamento
```
Gold â†’ Banco de Dados â†’ RelatÃ³rios â†’ APIs (futuro)
```

## Tratamento de Erros

### 1. Erros de Coleta
- **Timeout**: Retry automÃ¡tico com backoff exponencial
- **Rate Limiting**: Delay entre requisiÃ§Ãµes
- **Dados InacessÃ­veis**: Log de erro e continuaÃ§Ã£o com outras fontes

### 2. Erros de Processamento
- **Dados InvÃ¡lidos**: RemoÃ§Ã£o e log de erro
- **Falha de TransformaÃ§Ã£o**: Rollback e reprocessamento
- **Falha de Banco**: Retry e fallback para arquivo

### 3. Monitoramento
- **Logs Estruturados**: NÃ­veis INFO, WARNING, ERROR
- **MÃ©tricas de Qualidade**: Score em tempo real
- **Alertas**: Falhas crÃ­ticas (futuro)

## ConfiguraÃ§Ã£o e Deploy

### 1. ConfiguraÃ§Ã£o Local
```bash
# Instalar dependÃªncias
pip install -r requirements.txt

# Configurar ambiente
cp .env.example .env
# Editar .env com configuraÃ§Ãµes

# Executar setup
python setup.py

# Testar sistema
python main.py test
```

### 2. Deploy com Docker
```bash
# Build da imagem
docker build -t ship-lineup-pipeline .

# Executar com docker-compose
docker-compose up -d
```

### 3. Deploy em ProduÃ§Ã£o
- **Servidor**: Linux com Python 3.8+
- **Banco**: PostgreSQL recomendado
- **Monitoramento**: Logs + mÃ©tricas
- **Backup**: Automatizado diÃ¡rio

## Performance e Escalabilidade

### 1. OtimizaÃ§Ãµes Atuais
- **Parquet**: Formato otimizado para analytics
- **Ãndices**: Ãndices em campos de consulta frequente
- **Batch Processing**: Processamento em lotes
- **Connection Pooling**: Pool de conexÃµes de banco

### 2. LimitaÃ§Ãµes Conhecidas
- **Web Scraping**: Dependente da estrutura dos sites
- **Processamento Sequencial**: NÃ£o paralelizado
- **MemÃ³ria**: Carregamento completo de DataFrames

### 3. Melhorias Futuras
- **Processamento Paralelo**: MÃºltiplas fontes simultÃ¢neas
- **Streaming**: Processamento em tempo real
- **Cache**: Cache de dados frequentes
- **APIs**: SubstituiÃ§Ã£o de web scraping por APIs

## SeguranÃ§a

### 1. Medidas Implementadas
- **Rate Limiting**: Controle de requisiÃ§Ãµes
- **User-Agent**: IdentificaÃ§Ã£o adequada
- **Timeout**: PrevenÃ§Ã£o de travamentos
- **ValidaÃ§Ã£o**: SanitizaÃ§Ã£o de dados

### 2. ConsideraÃ§Ãµes
- **Dados PÃºblicos**: Apenas dados pÃºblicos sÃ£o coletados
- **Respeito Ã s Fontes**: Rate limiting e identificaÃ§Ã£o
- **Logs**: NÃ£o exposiÃ§Ã£o de dados sensÃ­veis

## Monitoramento e Observabilidade

### 1. Logs
- **Estruturados**: Formato JSON para parsing
- **RotaÃ§Ã£o**: DiÃ¡ria com retenÃ§Ã£o de 30 dias
- **NÃ­veis**: DEBUG, INFO, WARNING, ERROR

### 2. MÃ©tricas
- **Qualidade**: Score de qualidade de dados
- **Volume**: Quantidade de registros processados
- **Performance**: Tempo de processamento
- **Erros**: Taxa de erro por fonte

### 3. RelatÃ³rios
- **DiÃ¡rios**: Resumo de processamento
- **Qualidade**: RelatÃ³rio de qualidade de dados
- **AgregaÃ§Ãµes**: MÃ©tricas de negÃ³cio

## Extensibilidade

### 1. Novos Portos
```python
class NovoPortoCollector(BaseCollector):
    def collect_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        # Implementar coleta especÃ­fica
        pass
```

### 2. Novas Fontes
- Implementar interface BaseCollector
- Adicionar configuraÃ§Ã£o em Config.DATA_SOURCES
- Configurar validaÃ§Ãµes especÃ­ficas

### 3. Novas TransformaÃ§Ãµes
- Estender MedallionPipeline
- Adicionar regras de negÃ³cio
- Implementar novas mÃ©tricas

## Troubleshooting

### 1. Problemas Comuns

**Erro de Coleta**:
- Verificar conectividade
- Verificar se o site estÃ¡ acessÃ­vel
- Ajustar timeout se necessÃ¡rio

**Dados InvÃ¡lidos**:
- Verificar logs de validaÃ§Ã£o
- Ajustar regras de validaÃ§Ã£o
- Verificar mudanÃ§as na fonte

**Falha de Banco**:
- Verificar conexÃ£o
- Verificar espaÃ§o em disco
- Verificar permissÃµes

### 2. Comandos de DiagnÃ³stico
```bash
# Teste bÃ¡sico
python main.py test

# Verificar logs
tail -f logs/ship_lineup.log

# Verificar banco
python -c "from src.database.database_manager import DatabaseManager; db = DatabaseManager(); print(db.get_database_stats())"
```

## Roadmap TÃ©cnico

### Fase 1 (Atual)
- âœ… Arquitetura medallion
- âœ… Coletores bÃ¡sicos
- âœ… Pipeline ETL
- âœ… ValidaÃ§Ã£o de dados
- âœ… Agendamento

### Fase 2 (PrÃ³xima)
- ğŸ”„ API REST
- ğŸ”„ Dashboard web
- ğŸ”„ Alertas automÃ¡ticos
- ğŸ”„ Processamento paralelo

### Fase 3 (Futuro)
- ğŸ“‹ Machine Learning
- ğŸ“‹ IntegraÃ§Ã£o com BI
- ğŸ“‹ Cache distribuÃ­do
- ğŸ“‹ MicroserviÃ§os

